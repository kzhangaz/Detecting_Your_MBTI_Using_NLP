{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "df = pd.read_csv('../data/mbti_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/luanan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/luanan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/luanan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/Users/luanan/anaconda3/envs/csci1470/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Shape: (8675, 79874)\n",
      "TF-IDF Feature Names: ['aa' 'aaa' 'aaaa' ... 'ﾉｼ' 'ﾟ' 'ﾟдﾟщ']\n",
      "Word Vector for 'climate': [ 1.90252766e-01  1.48653913e+00 -3.71686339e-01 -2.38267437e-01\n",
      " -3.82821679e-01  2.07242668e-01 -6.41261637e-02  1.50640702e+00\n",
      "  5.11804104e-01 -2.56112039e-01 -1.93555343e+00  5.99927306e-01\n",
      " -6.67593300e-01 -1.69792816e-01  3.74819905e-01 -1.75895822e+00\n",
      "  1.54398731e-04 -2.61034757e-01 -8.36529732e-01  1.14265487e-01\n",
      "  1.39894664e+00 -4.06002641e-01 -6.70712531e-01  3.16157162e-01\n",
      "  1.17947578e+00  5.12195706e-01 -1.29408395e+00 -8.09257090e-01\n",
      " -1.35896909e+00  9.06167805e-01 -1.91821493e-02 -1.40571013e-01\n",
      "  1.88803303e+00  1.20748699e+00 -6.21329188e-01  6.91762924e-01\n",
      "  9.66809750e-01 -2.07818389e-01 -2.25952808e-02 -4.32303488e-01\n",
      "  2.39120558e-01 -1.57070351e+00 -2.52456486e-01 -4.93468612e-01\n",
      " -3.66193771e-01 -1.97554088e+00  6.96181297e-01  8.45668316e-01\n",
      " -5.95512629e-01  4.14425820e-01  1.51373878e-01  1.07751215e+00\n",
      "  1.53143597e+00 -1.95859993e+00 -6.67558730e-01  3.66720736e-01\n",
      " -2.59543419e-01 -1.18592703e+00 -8.13363135e-01 -3.78673106e-01\n",
      "  2.85383105e-01 -4.10687447e-01 -8.25646333e-04  1.14021540e+00\n",
      " -1.05826163e+00 -5.86983204e-01  3.20836455e-02 -9.29200709e-01\n",
      " -6.03626907e-01  1.39954090e+00  1.77925840e-01 -3.17309052e-01\n",
      " -4.64627557e-02  8.62324655e-01  9.46820319e-01  1.33343279e+00\n",
      " -5.99168465e-02 -2.08268136e-01  8.18966627e-02  1.08615899e+00\n",
      "  4.00642082e-02 -9.64649200e-01 -5.44691861e-01  4.58952427e-01\n",
      " -4.11536962e-01  1.73450604e-01  2.62449384e-01 -1.18265778e-01\n",
      "  1.02240121e+00  3.92450005e-01 -4.84888226e-01  4.75594729e-01\n",
      "  1.00536026e-01 -1.79207063e+00  4.92040306e-01  1.30182707e+00\n",
      "  1.03396714e+00 -1.45029676e+00 -1.19771230e+00  7.72531390e-01]\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing function\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Removes URLs that start with http\n",
    "    text = re.sub(r'www\\S+', '', text)   # Removes URLs that start with www\n",
    "\n",
    "    text = text.lower()  # Lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "df['processed_posts'] = df['posts'].apply(preprocess)\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['processed_posts']).toarray()\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Word2Vec\n",
    "word2vec_model = Word2Vec(sentences=df['processed_posts'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "word2vec_model.train(df['processed_posts'], total_examples=word2vec_model.corpus_count, epochs=10)\n",
    "\n",
    "# Example: Get the vector for a word\n",
    "word_vector = word2vec_model.wv['climate']  # Get the vector for the word \"climate\"\n",
    "\n",
    "# Show TF-IDF result\n",
    "print(\"TF-IDF Matrix Shape:\", tfidf_matrix.shape)\n",
    "print(\"TF-IDF Feature Names:\", tfidf_feature_names)\n",
    "\n",
    "# Example output from Word2Vec\n",
    "print(\"Word Vector for 'climate':\", word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tfidf embedding result\n",
    "np.save('../data/tfidf_matrix.npy', tfidf_matrix)\n",
    "\n",
    "# to load the tf-idf matrix:\n",
    "# loaded_matrix = np.load('tfidf_matrix.npy')\n",
    "\n",
    "# save word2vec model\n",
    "word2vec_model.save('../data/word2vec_model.gensim')\n",
    "# Load the model\n",
    "# df = Word2Vec.load('word2vec_model.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/mbti_1.csv\")\n",
    "\"\"\"\n",
    "Here we create 4 new columns each containing information about one of the key dichotomies of MBTI\n",
    "\"\"\"\n",
    "\n",
    "def label_mbti_ie(row):\n",
    "    if \"I\" in row['type']:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def label_mbti_ns(row):\n",
    "    if \"N\" in row['type']:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def label_mbti_ft(row):\n",
    "    if \"F\" in row['type']:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def label_mbti_pj(row):\n",
    "    if \"P\" in row['type']:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "df['I/E'] = df.apply(label_mbti_ie, axis=1)\n",
    "df['N/S'] = df.apply(label_mbti_ns, axis=1)\n",
    "df['F/T'] = df.apply(label_mbti_ft, axis=1)\n",
    "df['P/J'] = df.apply(label_mbti_pj, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.load(\"tfidf_matrix.npy\")\n",
    "embed = pd.DataFrame({'Row': [np.array(row)[1:-1] for row in embedding_matrix]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I/E</th>\n",
       "      <th>N/S</th>\n",
       "      <th>F/T</th>\n",
       "      <th>P/J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8675 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      I/E  N/S  F/T  P/J\n",
       "0       0    0    0    1\n",
       "1       1    0    1    0\n",
       "2       0    0    1    0\n",
       "3       0    0    1    1\n",
       "4       1    0    1    1\n",
       "...   ...  ...  ...  ...\n",
       "8670    0    1    0    0\n",
       "8671    1    0    0    0\n",
       "8672    0    0    1    0\n",
       "8673    0    0    0    0\n",
       "8674    0    0    0    0\n",
       "\n",
       "[8675 rows x 4 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "four_cat = df[['I/E', 'N/S','F/T','P/J']]\n",
    "four_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>I/E</th>\n",
       "      <th>N/S</th>\n",
       "      <th>F/T</th>\n",
       "      <th>P/J</th>\n",
       "      <th>Row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>ISFP</td>\n",
       "      <td>'https://www.youtube.com/watch?v=t8edHB_h908||...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>'So...if this thread already exists someplace ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'So many questions when i do these things.  I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'I am very conflicted right now when it comes ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'It has been too long since I have been on per...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8675 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                              posts  I/E  N/S  F/T  \\\n",
       "0     INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...    0    0    0   \n",
       "1     ENTP  'I'm finding the lack of me in these posts ver...    1    0    1   \n",
       "2     INTP  'Good one  _____   https://www.youtube.com/wat...    0    0    1   \n",
       "3     INTJ  'Dear INTP,   I enjoyed our conversation the o...    0    0    1   \n",
       "4     ENTJ  'You're fired.|||That's another silly misconce...    1    0    1   \n",
       "...    ...                                                ...  ...  ...  ...   \n",
       "8670  ISFP  'https://www.youtube.com/watch?v=t8edHB_h908||...    0    1    0   \n",
       "8671  ENFP  'So...if this thread already exists someplace ...    1    0    0   \n",
       "8672  INTP  'So many questions when i do these things.  I ...    0    0    1   \n",
       "8673  INFP  'I am very conflicted right now when it comes ...    0    0    0   \n",
       "8674  INFP  'It has been too long since I have been on per...    0    0    0   \n",
       "\n",
       "      P/J                                                Row  \n",
       "0       1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1       0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2       0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3       1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4       1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "...   ...                                                ...  \n",
       "8670    0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "8671    0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "8672    0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "8673    0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "8674    0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[8675 rows x 7 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.concat([df, embed], axis=1)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('../data/tfidf_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_cat.to_csv('../data/four_cat.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('csci1470')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "37ddd90401f1d77a01b8a047b2f4a1e6caea817e7c262c0d72d1b8dd241ceef0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
